{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc698c86-2562-425c-bbb3-111e802a7018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata==0.4.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: torchtext==0.13.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: torch==1.12.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: gensim in /home/jckuri/anaconda3/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torchdata==0.4.0) (1.25.11)\n",
      "Requirement already satisfied: requests in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torchdata==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torchdata==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torchtext==0.13.0) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torchtext==0.13.0) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions in /home/jckuri/anaconda3/lib/python3.8/site-packages (from torch==1.12.0) (4.7.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in /home/jckuri/anaconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (1.3.0)\n",
      "Requirement already satisfied: pyfume in /home/jckuri/anaconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from requests->torchdata==0.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jckuri/.local/lib/python3.8/site-packages (from requests->torchdata==0.4.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from requests->torchdata==0.4.0) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: simpful in /home/jckuri/anaconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: fst-pso in /home/jckuri/anaconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jckuri/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /home/jckuri/anaconda3/lib/python3.8/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m21.1.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torchdata==0.4.0 torchtext==0.13.0 torch==1.12.0 gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473163d2-1f9c-423d-9d29-dd8e4674bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset)=87599, len(valid_dataset)=10570\n",
      "len(train_dataset)=8000, len(valid_dataset)=2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import *\n",
    "from load_datasets import *\n",
    "from vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ca72c4-57e1-4a1e-b10f-bb96cafc0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=8000, word_count=12082\n",
      "n_samples=2000, word_count=13801\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "vocab.load(train_dataset)\n",
    "vocab.load(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ff01e1-a52d-443d-b8cf-3ca93c4f310d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceToSequence(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(13801, 400)\n",
       "    (lstm): LSTM(400, 400)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(13801, 400)\n",
       "    (lstm): LSTM(400, 400)\n",
       "    (output): Linear(in_features=400, out_features=13801, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = 'senquence_to_sequence.pt'\n",
    "\n",
    "model = torch.load(model_file)\n",
    "model.to(model.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9832d47-4bfa-4464-be03-9f0f49aca1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_string(tensor, vocab):\n",
    "    words = [vocab.index_to_word[tensor[i, 0].item()] for i in range(tensor.size(0))]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def tokens_to_string(tokens, vocab):\n",
    "    words = [vocab.index_to_word[token] for token in tokens]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def evaluate(question, vocab, model, max_length = 1000):\n",
    "    question = question.lower()\n",
    "    try:\n",
    "        question_tensor = sentence_to_tensor(vocab, question, model.device)\n",
    "    except:\n",
    "        return \"ERROR: A word was not found in the vocabulary.\"\n",
    "    tokens = []\n",
    "    output = model(question_tensor, None, question_tensor.size(0), max_length)\n",
    "    for output_tensor in output['decoder_output']:\n",
    "        _, top_token = output_tensor.data.topk(1)\n",
    "        output_token = top_token.item()\n",
    "        tokens.append(output_token)\n",
    "        if output_token == 1:\n",
    "            break            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_question(question, vocab, model):\n",
    "    question = question.lower()\n",
    "    try:\n",
    "        question_tensor = sentence_to_tensor(vocab, question, model.device)\n",
    "    except:\n",
    "        print(\"ERROR: A word was not found in the vocabulary.\")\n",
    "        return\n",
    "    print(\"QUESTION:\")\n",
    "    print(question)\n",
    "    processed_question = tensor_to_string(question_tensor, vocab)\n",
    "    print(processed_question)\n",
    "    question_tokens = [question_tensor[i, 0].item() for i in range(question_tensor.size(0))]\n",
    "    print(question_tokens)\n",
    "    print(\"ANSWER:\")\n",
    "    answer_tokens = evaluate(question, vocab, model)\n",
    "    answer = tokens_to_string(answer_tokens, vocab)\n",
    "    print(answer)\n",
    "    print(answer_tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5a7c59-b360-4d7a-879f-5915b8364f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = \\\n",
    " [\"Who are you?\",\n",
    "  \"What are you doing?\",\n",
    "  \"What is the capital of France?\",\n",
    "  \"What is your favorite color?\",\n",
    "  \"How was your day?\",\n",
    "  \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
    "  \"What is in front of the Notre Dame Main Building?\",\n",
    "  \"The Basilica of the Sacred heart at Notre Dame is beside to which structure?\",\n",
    "  \"What is the Grotto at Notre Dame?\",\n",
    "  \"What sits on top of the Main Building at Notre Dame?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92bbb20-c134-4469-9bd6-8374806f7eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "who are you?\n",
      "<SOS> who are you ? <EOS>\n",
      "[0, 366, 68, 1353, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> unspayed females <EOS>\n",
      "[0, 11884, 413, 1]\n",
      "\n",
      "QUESTION:\n",
      "what are you doing?\n",
      "<SOS> what are you doing ? <EOS>\n",
      "[0, 18, 68, 1353, 1882, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> parasites or omnivores <EOS>\n",
      "[0, 11882, 1361, 12010, 1]\n",
      "\n",
      "QUESTION:\n",
      "what is the capital of france?\n",
      "<SOS> what is the capital of france ? <EOS>\n",
      "[0, 18, 19, 5, 5131, 21, 13, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> the great vehicle <EOS>\n",
      "[0, 5, 1490, 10565, 1]\n",
      "\n",
      "QUESTION:\n",
      "what is your favorite color?\n",
      "<SOS> what is your favorite color ? <EOS>\n",
      "[0, 18, 19, 6109, 3947, 2073, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> my <EOS>\n",
      "[0, 1220, 1]\n",
      "\n",
      "QUESTION:\n",
      "how was your day?\n",
      "<SOS> how was your day ? <EOS>\n",
      "[0, 54, 107, 6109, 967, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> 10 <EOS>\n",
      "[0, 236, 1]\n",
      "\n",
      "QUESTION:\n",
      "to whom did the virgin mary allegedly appear in 1858 in lourdes france?\n",
      "<SOS> to whom did the virgin mary allegedly appear in 1858 in lourdes france ? <EOS>\n",
      "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 10, 12, 13, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> saint bernadette soubirous <EOS>\n",
      "[0, 15, 16, 17, 1]\n",
      "\n",
      "QUESTION:\n",
      "what is in front of the notre dame main building?\n",
      "<SOS> what is in front of the notre dame main building ? <EOS>\n",
      "[0, 18, 19, 10, 20, 21, 5, 22, 23, 24, 25, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> a copper statue of christ <EOS>\n",
      "[0, 26, 27, 28, 21, 29, 1]\n",
      "\n",
      "QUESTION:\n",
      "the basilica of the sacred heart at notre dame is beside to which structure?\n",
      "<SOS> the basilica of the sacred heart at notre dame is beside to which structure ? <EOS>\n",
      "[0, 5, 30, 21, 5, 31, 32, 33, 22, 23, 19, 34, 2, 35, 36, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> the main building <EOS>\n",
      "[0, 5, 24, 25, 1]\n",
      "\n",
      "QUESTION:\n",
      "what is the grotto at notre dame?\n",
      "<SOS> what is the grotto at notre dame ? <EOS>\n",
      "[0, 18, 19, 5, 37, 33, 22, 23, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> a marian place of prayer and reflection <EOS>\n",
      "[0, 26, 38, 39, 21, 40, 41, 42, 1]\n",
      "\n",
      "QUESTION:\n",
      "what sits on top of the main building at notre dame?\n",
      "<SOS> what sits on top of the main building at notre dame ? <EOS>\n",
      "[0, 18, 43, 44, 45, 21, 5, 24, 25, 33, 22, 23, 14, 1]\n",
      "ANSWER:\n",
      "<SOS> a golden statue of the virgin mary <EOS>\n",
      "[0, 26, 46, 28, 21, 5, 6, 7, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    process_question(question, vocab, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
